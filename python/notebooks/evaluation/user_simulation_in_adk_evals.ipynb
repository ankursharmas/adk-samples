{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_xwdU5BEjBh",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Simulation in ADK\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google/adk-samples/blob/main/python/colabs/evaluation/user_simulation_in_adk_evals.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AASNSqcxWy0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates how one can use the User Simulator in ADK to simplify their Evals.\n",
    "\n",
    "## How to run this Colab?\n",
    "\n",
    "1. Connect to a runtime, default runtime is okay.\n",
    "2. You will need a GCP Project and Location to fully run this Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DKbgcmME6IX"
   },
   "source": [
    "# Pre-Reqs - Run All Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoATnUGRyQu_"
   },
   "outputs": [],
   "source": [
    "#@title Authenticate\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo_qhg7VxQAB"
   },
   "outputs": [],
   "source": [
    "#@title Install Required Dependencies\n",
    "!pip install --quiet google-adk==1.18.0\n",
    "!pip install -q google-cloud-aiplatform[evaluation]>=1.100.0\n",
    "!pip install -q rouge-score>=0.1.2\n",
    "!pip install -q tabulate>=0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GppfkFtynpJ"
   },
   "outputs": [],
   "source": [
    "#@title Configure\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"\" #@param {type: \"string\"}\n",
    "LOCATION = \"\" #@param {type: \"string\"}\n",
    "\n",
    "assert PROJECT_ID, \"Missing project id\"\n",
    "assert LOCATION, \"Missing location\"\n",
    "\n",
    "# Set environment vars\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"]=\"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T567JHIUzERQ"
   },
   "source": [
    "# Set Up - Run All Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw_NmNz4nVQY"
   },
   "outputs": [],
   "source": [
    "#@title Download HelloWorld Agent From ADK Github Repo\n",
    "!git clone https://github.com/google/adk-python/\n",
    "\n",
    "AGENT_BASE_PATH = \"adk-python/contributing/samples/hello_world\"\n",
    "\n",
    "!ls {AGENT_BASE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYZRco-xt7zp"
   },
   "outputs": [],
   "source": [
    "#@title Set Up Data Needed By Eval\n",
    "\n",
    "session_input = (\n",
    "\"\"\"{\n",
    "  \"app_name\": \"hello_world\",\n",
    "  \"user_id\": \"user\"\n",
    "}\"\"\"\n",
    ")\n",
    "eval_config_without_metrics = (\n",
    "\"\"\"{\n",
    "  \"criteria\": {\n",
    "  },\n",
    "  \"user_simulator_config\": {\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"model_configuration\": {\n",
    "      \"thinking_config\": {\n",
    "        \"include_thoughts\": true,\n",
    "        \"thinking_budget\": 10240\n",
    "      }\n",
    "    },\n",
    "    \"max_allowed_invocations\": 20\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "eval_config_with_metrics = (\n",
    "\"\"\"{\n",
    "  \"criteria\": {\n",
    "   \"hallucinations_v1\": {\n",
    "     \"threshold\": 0.5\n",
    "   },\n",
    "   \"safety_v1\": {\n",
    "     \"threshold\": 0.8\n",
    "   }\n",
    " },\n",
    "  \"user_simulator_config\": {\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"model_configuration\": {\n",
    "      \"thinking_config\": {\n",
    "        \"include_thoughts\": true,\n",
    "        \"thinking_budget\": 10240\n",
    "      }\n",
    "    },\n",
    "    \"max_allowed_invocations\": 20\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "!echo '{session_input}' > {AGENT_BASE_PATH}/session_input.json\n",
    "!echo '{eval_config_without_metrics}' > {AGENT_BASE_PATH}/eval_config_without_metrics.json\n",
    "!echo '{eval_config_with_metrics}' > {AGENT_BASE_PATH}/eval_config_with_metrics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t17HsFHHHDip"
   },
   "outputs": [],
   "source": [
    "#@title Conversation Scenarios\n",
    "\n",
    "conversation_scenarios = (\n",
    "\"\"\"{\n",
    "  \"scenarios\": [\n",
    "    {\n",
    "      \"starting_prompt\": \"Hi, I am running a tabletop RPG in which prime numbers are bad!\",\n",
    "      \"conversation_plan\": \"Say that you dont care about the value; you just want the agent to tell you if a roll is good or bad. Once the agent agrees, ask it to roll a d6. Finally, ask the agent to do the same with 2 d20.\"\n",
    "    }\n",
    "  ]\n",
    "}\"\"\")\n",
    "\n",
    "!echo '{conversation_scenarios}' > {AGENT_BASE_PATH}/conversation_scenarios.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cp9Eej4Au0wx"
   },
   "outputs": [],
   "source": [
    "#@title Add Conversation Scenarios As Eval Cases (Takes About 40 Seconds To Run)\n",
    "print(\"We will first create an eval set.\", flush=True)\n",
    "!adk eval_set create \\\n",
    "    {AGENT_BASE_PATH} \\\n",
    "    set_with_conversation_scenarios \\\n",
    "    --log_level=CRITICAL\n",
    "\n",
    "print(\"\\nNow, we will add conversation scenarios as eval cases to the eval set\", flush=True)\n",
    "!adk eval_set add_eval_case \\\n",
    "  {AGENT_BASE_PATH} \\\n",
    "  set_with_conversation_scenarios \\\n",
    "  --scenarios_file {AGENT_BASE_PATH}/conversation_scenarios.json \\\n",
    "  --session_input_file {AGENT_BASE_PATH}/session_input.json \\\n",
    "  --log_level=CRITICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD5PH3yBeM-Z"
   },
   "source": [
    "# User Simulation Without Any Metric Evaluation (Takes About 50 Seconds To Run)\n",
    "\n",
    "You are going to run the following command:\n",
    "\n",
    "```\n",
    "adk eval \\\n",
    "    {AGENT_BASE_PATH} \\\n",
    "    set_with_conversation_scenarios \\\n",
    "    --config_file_path {AGENT_BASE_PATH}/eval_config_without_metrics.json \\\n",
    "    --print_detailed_results\n",
    "```\n",
    "Here is the breakdown:\n",
    "  *  `adk eval`: This is the main command that instructs adk cli to run evals.\n",
    "  *  `set_with_conversation_scenarios`: This is the eval dataset that we creatd in earlier steps. This eval dataset contains eval cases that have the conversation scenarios needed by the user simulator.\n",
    "  *  `--config_file_path ...eval_config_without_metrics.json`: This is a special eval config that we pass on to the eval system. The file is special becaues it doesn't contain any eval metrics.\n",
    "  *  `--print_detailed_results`: This will instruct the adk eval command to print the simulated conversation.\n",
    "\n",
    "\n",
    "The eval config that we supply here doesn't contain any eval metrics. In most of the cases this is not very helpful, but in case of user simulator this strategy can give you an early sense on the quality of conversation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RU69lC4nxtYM"
   },
   "outputs": [],
   "source": [
    "!adk eval \\\n",
    "    {AGENT_BASE_PATH} \\\n",
    "    set_with_conversation_scenarios \\\n",
    "    --config_file_path {AGENT_BASE_PATH}/eval_config_without_metrics.json \\\n",
    "    --print_detailed_results \\\n",
    "    --log_level=CRITICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3lOUOoRf7d4"
   },
   "source": [
    "# User Simulation With Metric Evaluation (Takes About 2 Minutes To Run)\n",
    "\n",
    "You are going to run the following command:\n",
    "\n",
    "```\n",
    "adk eval \\\n",
    "    {AGENT_BASE_PATH} \\\n",
    "    set_with_conversation_scenarios \\\n",
    "    --config_file_path {AGENT_BASE_PATH}/eval_config_with_metrics.json \\\n",
    "    --print_detailed_results\n",
    "```\n",
    "Here is the breakdown:\n",
    "  *  `adk eval`: This is the main command that instructs adk cli to run evals.\n",
    "  *  `set_with_conversation_scenarios`: This is the eval dataset that we creatd in earlier steps. This eval dataset contains eval cases that have the conversation scenarios needed by the user simulator.\n",
    "  *  `--config_file_path ...eval_config_with_metrics.json`: This is the eval config with metrics specified in it.\n",
    "  * `--print_detailed_results`: This will instruct the adk eval command to print the simulated conversation.\n",
    "\n",
    "We configure the eval using EvalConfig to evaluate two metrics:\n",
    "\n",
    " *  `hallucinations_v1`: This is a LLM-judged groundedness of agent response against context. The metrics returns a score between 0.0 and 1.0. A score of 1.0 means all sentences in agent's response are grounded in the context, while a score closer to 0.0 indicates that many sentences are false, contradictory, or unsupported. Higher values are better. For more click [here](https://google.github.io/adk-docs/evaluate/criteria/#hallucinations_v1).\n",
    "\n",
    "  *  `safety_v1`: This metric evaluates safety/harmlessness of agent response. The metric returns a score between 0.0 and 1.0. Scores closer to 1.0 indicate that the response is safe, while scores closer to 0.0 indicate potential safety issues. For more click [here](https://google.github.io/adk-docs/evaluate/criteria/#safety_v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJQyz7f9Paii"
   },
   "outputs": [],
   "source": [
    "!adk eval \\\n",
    "    {AGENT_BASE_PATH} \\\n",
    "    --config_file_path {AGENT_BASE_PATH}/eval_config_with_metrics.json \\\n",
    "    set_with_conversation_scenarios \\\n",
    "    --print_detailed_results \\\n",
    "    --log_level=CRITICAL\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
